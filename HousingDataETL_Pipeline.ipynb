{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "15f590af-91f8-4dfa-b59a-37f02458aa30",
   "metadata": {
    "tags": []
   },
   "source": [
    "# UAD (Uniform Appraisal Data) Data ETL\n",
    "\n",
    "Federal Housing Finance Agency (FHFA) releases housing appraisal dataset (anually/quarterly). This dataset is aggregated from home value appraisals for refinance and/or purchase. UAD dataset can provide insights into local appraised house prices.\n",
    "\n",
    "Obviously there are some caveats to using UAD data as a represntative for house prices. See the following link:\n",
    "\n",
    "https://www.fhfa.gov/Media/Blog/Pages/Exploring-Appraisal-Bias-Using-UAD-Aggregate-Statistics.aspx\n",
    "\n",
    "**Notes on the raw data**\n",
    "\n",
    "* UAD Data is aggregated by census tract.\n",
    "* There were some updates in census tract 2010 vs. census tract 2020.\n",
    "* Mapping between zip code and census tract is not one-to-one.\n",
    "\n",
    "## Based on the aforementioned facts, following steps were performed to get approximate home appraisal values for each zip codes:\n",
    "\n",
    "* UAD dataset gives data based on census tract 2020 census tract.\n",
    "* HUD provides map from census tract 2010 to US zip codes.\n",
    "* The relationship between census tract 2010 to census tract 2020 is provided by census.gov.\n",
    "\n",
    "* We will map (UAD dataset census tract 2020) to (census tract 2010) to (USPS Zip Code). ***These mappings are not one-to-one. To simplify our analysis, we only keep the first x-to-y map and drop other x-to-y1, x-to-y2 maps, essentailly making the relationship one-to-one. Our justification is - cenusus tracts will map to nearby zip codes and therefore, even if we are wrong in our x-to-y mapping, the close vicinity of y1, y2 still allows us to approximate localized home value appraisal.***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5106c49a-50e0-40a3-96df-d6676f0c3414",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Downloading relevant data files (if not present)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ab4818b5-81cc-4993-b8b7-4944ab410068",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to check for data file(s) and fetch data them, if not present.\n",
    "\n",
    "import requests, os, time\n",
    "\n",
    "def fetch_file(url, folder_name, presence_check=True,  fname=None):\n",
    "    \"\"\" Fetches file from URL.\n",
    "    Input:\n",
    "      fname: File name to save If not provided, file name is derived from the provided URL. String part after the last '/' in the url.\n",
    "      url: Url to download.\n",
    "      presence_check: When true - Will check for the file's presence and if present, file won't be downloaded. Default value is true. If you want to overwrite existing value, please pass False to this parameter.\n",
    "    \n",
    "    Return:\n",
    "      Saves file at the specified filepath. When presence_check is True and file is already present, prints \"File already present.\" statement.\n",
    "    \"\"\"  \n",
    "    if fname is None:\n",
    "        fname = url.split('/')[-1]\n",
    "        fname = fname.split('?')[0]\n",
    "        \n",
    "    fpath = folder_name + '/' + fname\n",
    "\n",
    "    if presence_check and os.path.exists(f'{fpath}'): #TODO: File size check validation.\n",
    "        return print('File already present.')\n",
    "\n",
    "    if not os.path.isdir(folder_name):\n",
    "        os.makedirs(folder_name)\n",
    "\n",
    "    response = requests.get(url, stream=True)\n",
    "    time.sleep(2)\n",
    "    total_length = response.headers.get('content-length')\n",
    "    total_length = round(int(total_length)/1e6,2)\n",
    "    \n",
    "    print(f'To be saved as {fpath}. Total size to be written is: {total_length} MB') \n",
    "\n",
    "    with open(fpath, \"wb\") as f:\n",
    "        for chunk in response.iter_content(chunk_size=512):\n",
    "            if chunk:  # filter out keep-alive new chunks\n",
    "                f.write(chunk)\n",
    "    \n",
    "    if os.path.exists(f'{fpath}'): #TODO: File size check validation.\n",
    "        return print(f'File download succesful for {fname}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a183f6fa-f501-4c8c-bfd2-0882473ca270",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get UAD urls to download\n",
    "\n",
    "import configparser\n",
    "\n",
    "config = configparser.ConfigParser()\n",
    "config.read('hpa.cfg')\n",
    "\n",
    "uad_url = config.get('UAD', 'uad_url')\n",
    "tract_2_zip_url = config.get('UAD', 'tract_2_zip_url')\n",
    "tract20_10_map_url = config.get('UAD', 'tract20_10_map_url')\n",
    "zip_url = config.get('UAD', 'zip_url')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1b6c66ca-fe30-4434-96d1-a2eceae0f055",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already present.\n",
      "File already present.\n",
      "File already present.\n",
      "File already present.\n"
     ]
    }
   ],
   "source": [
    "# Download URLs \n",
    "fetch_file(folder_name='data/raw_data/FHFA-UAD',url=uad_url)\n",
    "fetch_file(folder_name='data/raw_data/HUD-USPS', url=tract_2_zip_url)\n",
    "fetch_file(folder_name='data/raw_data/census', url=tract20_10_map_url)\n",
    "fetch_file(folder_name='data/raw_data/geonames', url=zip_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b747023-6e68-4cd0-bea5-e32cb5e8803a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Loading Files - Data Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "19bbdd0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UAD data loaded successfully.\n",
      "Mapping data from tract_2010 to zip code loaded.\n",
      "Census tract 2020 to census tract 2010 map loaded.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load UAD data, drop rows with Null appriasal values in the VALUE column.\n",
    "uad_df = pd.read_csv('data/raw_data/FHFA-UAD/UADAggs_tract.zip')\n",
    "print('UAD data loaded successfully.')\n",
    "uad_df = uad_df[uad_df.VALUE.notna()]\n",
    "\n",
    "# Load tract to zip code data, drop one-to-many relationships\n",
    "tract_2_zip = pd.read_excel('data/raw_data/HUD-USPS/TRACT_ZIP_122021.xlsx')\n",
    "tract_2_zip = tract_2_zip.drop_duplicates(subset='tract')\n",
    "print('Mapping data from tract_2010 to zip code loaded.')\n",
    "\n",
    "# Load tract 2010 to tract 2020 map, select relevant colums and drop one-to-many relationships\n",
    "tract_2010_to_tract_2020 = pd.read_csv('data/raw_data/census/tab20_tract20_tract10_natl.txt', sep='|')\n",
    "print('Census tract 2020 to census tract 2010 map loaded.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "950724e1-ef5d-4cde-a62b-5fdda22c78ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data selection, drop duplicates\n",
    "tract_2020_2010_map = tract_2010_to_tract_2020[['GEOID_TRACT_20','GEOID_TRACT_10']]\n",
    "tract_2020_2010_map = tract_2020_2010_map.drop_duplicates(subset='GEOID_TRACT_20')\n",
    "\n",
    "# Maping census tract 2020 to census tract 2010.\n",
    "uad_df = uad_df.merge(tract_2020_2010_map, left_on='TRACT', right_on='GEOID_TRACT_20', how='left')\n",
    "\n",
    "# Dropping Null entries on GEOID_TRACT_20, if any\n",
    "uad_df = uad_df[uad_df.GEOID_TRACT_20.notna()]\n",
    "\n",
    "# Column data type assignment\n",
    "\n",
    "uad_df.GEOID_TRACT_10 = uad_df.GEOID_TRACT_10.astype('int64')\n",
    "uad_df.GEOID_TRACT_20 = uad_df.GEOID_TRACT_20.astype('int64')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0d07ba0-3493-46ea-bf63-8a2b20562aef",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Census Track to zip map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "82e0ca52-da46-44aa-97c9-e7c51500de35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapping Zip to Census tract 2010\n",
    "uad_df_merged = uad_df.merge(tract_2_zip[['tract', 'zip']], left_on = 'GEOID_TRACT_10', right_on='tract', how='left')\n",
    "\n",
    "# Dropping null Zip entries.\n",
    "uad_df_merged = uad_df_merged[uad_df_merged.zip.notna()].copy(deep=True)\n",
    "\n",
    "# Data type conversion\n",
    "uad_df_merged.zip = uad_df_merged.zip.astype(int)\n",
    "\n",
    "# Dropping dupllicate columns\n",
    "uad_df_merged.drop(columns='tract', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ad18224-3d08-4917-a349-20b6958b9a15",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Exploration Final UAD DF Merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8a7f8cd4-1581-483b-ad6b-fd556fc89195",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Both         615689\n",
       "Purchase     459814\n",
       "Refinance    459743\n",
       "Name: PURPOSE, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uad_df_merged[uad_df_merged.SERIESID == 'MEAN'].PURPOSE.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a8bdcba-44b4-4293-a800-df6c90298148",
   "metadata": {},
   "source": [
    "As per documentation, Purchase + Refinance = Both.\n",
    "\n",
    "However, it appears that there is an imbalance. I have found some entries with only *Both* values.\n",
    "Perhaps this is due to:\n",
    "* Record suppression - to make records anonymous\n",
    "* Incomplete data - data does not distinguish between Purchase/Refinance or suggest both!\n",
    "\n",
    "**For our puroposes, we should get both only. At least, initially!**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a2ead34-e5d4-48f0-954e-5c245e78ebde",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Selecting relevant columns, and save final merged UAD table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f05a00fe-7b9c-4c65-9a9a-06d5d3438763",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting only relevant columns\n",
    "uad_df_merged_slice_to_save =  uad_df_merged[['SERIESID', 'PURPOSE', 'YEAR', 'VALUE', 'GEOID_TRACT_10', 'GEOID_TRACT_20', 'zip']]\n",
    "\n",
    "# Filtering dataframe based on relevant SERIESIDs.\n",
    "uad_df_merged_slice_to_save = uad_df_merged_slice_to_save[uad_df_merged_slice_to_save.SERIESID.isin(['COUNT', 'MEDIAN', 'P25', 'P75', 'MEAN'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "569d521d-d6ee-44d6-8825-aac357d09959",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting UAD table into count, mean, median, p25 and p75 tables\n",
    "count_table = uad_df_merged_slice_to_save[(uad_df_merged_slice_to_save.PURPOSE == 'Both')\n",
    "                           & (uad_df_merged_slice_to_save.SERIESID == 'COUNT')]\n",
    "\n",
    "mean_table = uad_df_merged_slice_to_save[(uad_df_merged_slice_to_save.PURPOSE == 'Both')\n",
    "                           & (uad_df_merged_slice_to_save.SERIESID == 'MEAN')]\n",
    "\n",
    "median_table = uad_df_merged_slice_to_save[(uad_df_merged_slice_to_save.PURPOSE == 'Both')\n",
    "                           & (uad_df_merged_slice_to_save.SERIESID == 'MEDIAN')]\n",
    "\n",
    "p25_table = uad_df_merged_slice_to_save[(uad_df_merged_slice_to_save.PURPOSE == 'Both')\n",
    "                           & (uad_df_merged_slice_to_save.SERIESID == 'P25')]\n",
    "\n",
    "p75_table = uad_df_merged_slice_to_save[(uad_df_merged_slice_to_save.PURPOSE == 'Both')\n",
    "                           & (uad_df_merged_slice_to_save.SERIESID == 'P75')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "67b0f6d1-916c-4baf-9b11-a00209fe4d2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.26590535156548195"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking table statistics\n",
    "count_table.drop_duplicates(subset=['zip', 'YEAR']).shape[0]/count_table.drop_duplicates(subset=['GEOID_TRACT_20', 'YEAR']).shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f613fc65-7be5-4903-a5b7-9355c36151ce",
   "metadata": {},
   "source": [
    "Conclusion: ~75% of the zip codes repeated. Perhaps, this is due to the drop_duplicates method applied in the track_to_zip dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dd564d44-0d00-4715-928e-937935aa0dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Performing appropriate aggregation operation on count, mean, median, p25 and p75 tables. \n",
    "\n",
    "zip_count_table = count_table[['zip', 'YEAR', 'VALUE']].groupby(['zip', 'YEAR']).sum('VALUE').rename(columns={'VALUE':'VALUE_count'})\n",
    "zip_mean_table = mean_table[['zip', 'YEAR', 'VALUE']].groupby(['zip', 'YEAR']).mean('VALUE').rename(columns={'VALUE':'VALUE_mean'})\n",
    "zip_median_table = median_table[['zip', 'YEAR', 'VALUE']].groupby(['zip', 'YEAR']).mean('VALUE').rename(columns={'VALUE':'VALUE_median'})\n",
    "zip_p25_table = p25_table[['zip', 'YEAR', 'VALUE']].groupby(['zip', 'YEAR']).mean('VALUE').rename(columns={'VALUE':'VALUE_p25'})\n",
    "zip_p75_table = p75_table[['zip', 'YEAR', 'VALUE']].groupby(['zip', 'YEAR']).mean('VALUE').rename(columns={'VALUE':'VALUE_p75'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c495b6f2-e3a0-4800-a3a5-454e95d41834",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combining zip count, mean, median, p25 and p75 tables into single table where UAD data is turned into single table. This table is the end table of the ETL process for UAD data\n",
    "from functools import reduce\n",
    "\n",
    "dfs_to_merge = [zip_count_table, zip_mean_table, zip_median_table, zip_p25_table, zip_p75_table]\n",
    "\n",
    "zip_uad_df_merged = reduce(lambda  left,right: pd.merge(left,right,left_index=True, right_index=True,\n",
    "                                            how='left'), dfs_to_merge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "994310c0-02e8-4d4d-9a72-b8bd934be3e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save final UAD data table\n",
    "\n",
    "def save_pd_to_parquet(dtframe, fldr_name, table_name):\n",
    "    \"\"\"\n",
    "    This function saves dataframe as parquet file at specified folder locations. \n",
    "    Input:\n",
    "        fldr_name: Folder name where data will be saved. Sub-directory supported. For example, you can specificy \"destination_folder\" or you can specify \"destination_folder/yet_another_folder\".\n",
    "        table_name: This is the table name for parquet file(s). Data will be saved in a subdirectory under specified fldr_name with actual .parquet file with a timestamp. For example, \n",
    "            if table name is specified as \"abc\" then the folder organization will be \n",
    "                        | - fldr_name\n",
    "                        | -- abc\n",
    "                        | ---- abc_{os.timestamp}.parquet\n",
    "        dtframe: Input dataframe.\n",
    "    Return(s):\n",
    "        print statement saying data write was sucessful.\n",
    "    \"\"\"\n",
    "    import os\n",
    "    from datetime import datetime\n",
    "    cur_time = str(datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\"))\n",
    "    file_path = fldr_name + '/' + table_name\n",
    "    if not os.path.isdir(file_path):\n",
    "        os.makedirs(file_path)\n",
    "    dtframe.to_parquet(path=f'{fldr_name}/{table_name}/{table_name}_{cur_time}.parquet')\n",
    "    return print(f'Table:  {table_name} saved.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6273dc6e-c31f-4d31-8436-ce7323cd03e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table:  zip_uad_table saved.\n"
     ]
    }
   ],
   "source": [
    "# Save file to specified directory\n",
    "\n",
    "out_dir = 'data/etl_data/uad_appraisal'\n",
    "save_pd_to_parquet(dtframe=zip_uad_df_merged, fldr_name=out_dir, table_name='zip_uad_table')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d65f415-563c-47ef-9c65-26e686f8f55f",
   "metadata": {},
   "source": [
    "**NOTE**: Caveats on census tract to zip code mapping.\n",
    "\n",
    "The relationship between census tract and zip code is not often one-to-one. Here though, for our purposes, we approximated the relationship to be one-to-one (and dropped *duplicated* relations). Justifications:\n",
    "\n",
    "* We are interested in local prices. For one-to-many census tract to zip code relations, multiple zip codes that map to the same census tract are adjacent. For our purposes, mis-assignment of a zip code to its neigbouring one - while not ideal - is not detrimental."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c6dbaeb-3e6e-4f83-b4c5-b01d8f8aecb2",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Realtor Data\n",
    "\n",
    "Realtor.com provides real estate data *monthly/weekly* at https://www.realtor.com/research/data/.\n",
    "\n",
    "Here, we are retrieving the monthly data and convert that into yearly data for each of the zip codes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "611dfb78-5bf2-4521-b48c-96626661b391",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Download Realtor Data (if not present already)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "32f87eb3-1e13-4727-81b9-454cf4530eb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already present.\n"
     ]
    }
   ],
   "source": [
    "realtor_url = config.get('REALTOR', 'realtor_url')\n",
    "\n",
    "fetch_file(url=realtor_url, folder_name='data/raw_data/realtor_data/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ade69936-47da-417a-a653-e9485ca2fa31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Realtor.com data successfully read from the .csv file.\n"
     ]
    }
   ],
   "source": [
    "# Reading data from downoaded zip file\n",
    "realtor_df = pd.read_csv('data/raw_data/realtor_data/RDC_Inventory_Core_Metrics_Zip_History.csv', low_memory=False)\n",
    "print('Realtor.com data successfully read from the .csv file.')\n",
    "\n",
    "realtor_df = realtor_df.iloc[0:-1] # Dropping last line that contains aggregated summary (line Total)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "766c029c-f79b-486a-9b3b-8cd7851dc40c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting relevant columns.\n",
    "\n",
    "realtor_cols = ['month_date_yyyymm', 'postal_code', 'median_listing_price', 'average_listing_price', 'active_listing_count', 'median_days_on_market', 'new_listing_count', \n",
    "                'price_increased_count', 'price_reduced_count', 'pending_listing_count', 'median_listing_price_per_square_foot', 'median_square_feet', 'total_listing_count', \n",
    "               'pending_ratio', 'quality_flag']\n",
    "\n",
    "realtor_df_slice = realtor_df[realtor_cols].copy(deep=True)\n",
    "\n",
    "# Data format conversion\n",
    "realtor_df_slice.month_date_yyyymm = pd.to_datetime(realtor_df_slice.month_date_yyyymm, format='%Y%m') #.month_date_yyyymm.astype('datetime64[ns]')\n",
    "realtor_df_slice['postal_code'] = realtor_df_slice.postal_code.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a053d4e3-8243-4a1d-8481-f205ec7d7e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zip-Year aggregation\n",
    "\n",
    "realtor_df_slice_agg = realtor_df_slice.groupby(by=['postal_code', realtor_df_slice.month_date_yyyymm.dt.year]).agg(\n",
    "                                                                                            median_list_price = ('median_listing_price', 'mean'),\n",
    "                                                                                            avg_list_price = ('average_listing_price', 'mean'),\n",
    "                                                                                            active_list_count = ('active_listing_count', 'sum'),\n",
    "                                                                                            median_DOM = ('median_days_on_market', 'mean'),\n",
    "                                                                                            new_list_count = ('new_listing_count', 'sum'),\n",
    "                                                                                            price_increase_count = ('price_increased_count', 'sum'),\n",
    "                                                                                            price_reduced_count = ('price_reduced_count', 'sum'),\n",
    "                                                                                            pending_list_count = ('pending_listing_count', 'sum'),\n",
    "                                                                                            median_list_price_per_square_foot = ('median_listing_price_per_square_foot', 'mean'),\n",
    "                                                                                            median_square_feet = ('median_square_feet', 'mean'),\n",
    "                                                                                            total_list_count = ('total_listing_count', 'sum'),\n",
    "                                                                                            pending_ratio = ('pending_ratio', 'mean')\n",
    "                                                                                            )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f1e11a6-04de-4718-ac0e-2ccbe6684583",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Redfin data\n",
    "\n",
    "Redfin also releases data on housing market at https://www.redfin.com/news/data-center/."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4eef6538-0c94-4a61-89c7-4c27b9f49516",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already present.\n"
     ]
    }
   ],
   "source": [
    "import configparser\n",
    "\n",
    "config = configparser.ConfigParser()\n",
    "config.read('hpa.cfg')\n",
    "\n",
    "# Download redfin data\n",
    "\n",
    "redfin_url = config.get('REDFIN', 'redfin_url')\n",
    "\n",
    "fetch_file(url=redfin_url, folder_name='data/raw_data/redfin_data/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2850fbe0-fd09-49ab-815a-cee5e68d9fba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading redfin data from downloaded compressed file.\n",
    "redfin_df = pd.read_csv('data/raw_data/redfin_data/zip_code_market_tracker.tsv000.gz', sep='\\t')\n",
    "\n",
    "# Choosing relevant columns\n",
    "column_subset = ['period_end', 'property_type', 'median_sale_price', 'median_list_price', \n",
    "                'median_ppsf', 'homes_sold', 'pending_sales', 'new_listings',  'inventory',\n",
    "                'avg_sale_to_list', 'region' ]\n",
    "redfin_df_slice = redfin_df[column_subset].copy(deep=True)\n",
    "\n",
    "# Data reformating/ type conversion\n",
    "redfin_df_slice['zip'] = redfin_df_slice.region.str.split(': ', expand=True)[1].astype('int')\n",
    "redfin_df_slice.period_end = pd.to_datetime(redfin_df_slice.period_end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "517b19b5-f6aa-4570-9208-b90010cf033e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregating data on zip code, year.\n",
    "redfin_df_slice_zip_agg = redfin_df_slice.groupby(by=['zip', redfin_df_slice.period_end.dt.year]).agg(  median_sale_price = ('median_sale_price', 'mean'),\n",
    "                                                                                                        median_list_price = ('median_list_price', 'mean'),\n",
    "                                                                                                        median_ppsf = ('median_ppsf', 'mean'),\n",
    "                                                                                                        homes_sold = ('homes_sold', 'sum'),\n",
    "                                                                                                        pending_sales = ('pending_sales', 'sum'),\n",
    "                                                                                                        new_listings = ('new_listings', 'sum'),\n",
    "                                                                                                        inventory = ('inventory', 'sum'),\n",
    "                                                                                                        avg_sale_to_list = ('avg_sale_to_list', 'mean')\n",
    "                                                                                                     )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e34b0d2f-fb6f-4865-b9da-f80c400d306e",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Zillow Data\n",
    "\n",
    "Zillow releases research data at https://www.zillow.com/research/data/.\n",
    "\n",
    "Zillow House Value Index (dollar-dominated) seemed the most comprehsive for zip-code-based dataset. If one is only interested in metro areas, they have more options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "418856a4-0146-4d08-b7ca-8c6deb9c4c69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already present.\n"
     ]
    }
   ],
   "source": [
    "import configparser\n",
    "\n",
    "config = configparser.ConfigParser()\n",
    "config.read('hpa.cfg')\n",
    "\n",
    "# Get zillow data\n",
    "zillow_url = config.get('ZILLOW', 'zillow_url')\n",
    "\n",
    "fetch_file(url=zillow_url, folder_name='data/raw_data/zillow_data/', fname='Zip_zhvi_uc_sfrcondo_tier_0.33_0.67_sm_sa_month.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f5bb304a-af49-48ab-92a8-2d75a0a8197e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zillow data read succesful.\n"
     ]
    }
   ],
   "source": [
    "# Reading zillow Data from downloaded file\n",
    "import pandas as pd\n",
    "zillow_zhvi_df = pd.read_csv('data/raw_data/zillow_data/Zip_zhvi_uc_sfrcondo_tier_0.33_0.67_sm_sa_month.csv')\n",
    "print(\"zillow data read succesful.\")\n",
    "\n",
    "# Selecting columns of interest\n",
    "excluded_cols = ['RegionID', 'SizeRank','StateName', 'State','RegionType' ]\n",
    "zillow_zhvi_df_slice = zillow_zhvi_df[[x for x in zillow_zhvi_df.columns if x not in excluded_cols]].copy(deep=True)\n",
    "zillow_zhvi_df_slice = zillow_zhvi_df_slice.set_index(['RegionName', 'City', 'Metro', 'CountyName']).stack().reset_index()\n",
    "zillow_zhvi_df_slice.columns = ['zip', 'city', 'metro', 'county', 'date', 'zhvi_usd_dominated']\n",
    "\n",
    "# Data type conversion\n",
    "zillow_zhvi_df_slice.date = pd.to_datetime(zillow_zhvi_df_slice.date)\n",
    "\n",
    "# Aggregate data based on zip code and year\n",
    "zillow_zhvi_zip_agg = zillow_zhvi_df_slice.groupby(by=['zip', zillow_zhvi_df_slice.date.dt.year]).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5035a923-6869-44ee-9bfe-bc046c91985c",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Aggregating Zip-Price Data from All Sources\n",
    "\n",
    "Extracted and transformed data from four sources are loaded into the final table:\n",
    "\n",
    "* Apprisal data from FHFA\n",
    "* Research data from redfin\n",
    "* Research data from realtor\n",
    "* Research data from Zillow \n",
    "\n",
    "The final table is saved as a parquet file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9c165592-531b-42ed-9b30-ea932c22da11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Harmonizing column headers for joining\n",
    "multi_index_names = ['zip', 'year']\n",
    "zip_uad_df_merged.index.names = multi_index_names\n",
    "redfin_df_slice_zip_agg.index.names = multi_index_names\n",
    "realtor_df_slice_agg.index.names = multi_index_names\n",
    "zillow_zhvi_zip_agg.index.names = multi_index_names\n",
    "\n",
    "# Adding suffix to column names to keep track of the source in the merged dataframe\n",
    "zip_uad_df_merged = zip_uad_df_merged.add_suffix('_uad')\n",
    "redfin_df_slice_zip_agg = redfin_df_slice_zip_agg.add_suffix('_redfin')\n",
    "realtor_df_slice_agg = realtor_df_slice_agg.add_suffix('_realtor')\n",
    "zillow_zhvi_zip_agg = zillow_zhvi_zip_agg.add_suffix('_zillow')\n",
    "\n",
    "# Merging dataframe\n",
    "dfs_to_merge = [zip_uad_df_merged, redfin_df_slice_zip_agg, realtor_df_slice_agg, zillow_zhvi_zip_agg]\n",
    "zip_price_master_df = reduce(lambda  left,right: pd.merge(left,right,left_index=True, right_index=True,\n",
    "                                            how='outer'), dfs_to_merge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9e948694-0da7-4b87-9bde-63a5c3d2727f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table:  house_price_table saved.\n"
     ]
    }
   ],
   "source": [
    "# Saving final table as parquet file\n",
    "out_dir = 'data/etl_data/zip_year_house_price_table'\n",
    "save_pd_to_parquet(dtframe=zip_price_master_df, fldr_name=out_dir, table_name='house_price_table')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d944aab-12f4-4775-aa34-03d857d01b4d",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Zip Code Details\n",
    "\n",
    "Source of data: http://download.geonames.org/export/zip/\n",
    "\n",
    "This table provides lattitude, longitude of each zip codes. This table can be used to map zip code(s) to specific lattitude(s), longitude(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "dc5eb29a-26da-446e-91a2-e1ff67a116f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already present.\n"
     ]
    }
   ],
   "source": [
    "import configparser\n",
    "\n",
    "config = configparser.ConfigParser()\n",
    "config.read('hpa.cfg')\n",
    "\n",
    "# Download Zip Code details\n",
    "\n",
    "zip_url = config.get('ZIPCODE', 'zip_url') \n",
    "\n",
    "fetch_file(url=zip_url, folder_name='data/raw_data/geonames/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b7c488e6-90f5-48ea-8d25-6b16b4a8a7d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data\n",
    "global_postcode_data = pd.read_csv('data/raw_data/geonames/allCountries.zip', sep='\\t', low_memory=False, header=None)\n",
    "column_headers = ['country_code', 'postal_code', 'place_name', 'admin_name1', 'admin_code1', 'admin_name2', 'admin_code2', 'admin_name3', 'admin_code3', 'latitude', 'longitude', 'accuracy']\n",
    "global_postcode_data.columns = column_headers\n",
    "\n",
    "# Getting only US zip codes\n",
    "us_postal_codes = global_postcode_data[global_postcode_data.country_code == 'US'].copy(deep=True)\n",
    "\n",
    "# Selecting relevant columns and renaming for clarity.\n",
    "us_postal_codes = us_postal_codes[['postal_code', 'place_name', 'admin_code1', 'latitude', 'longitude', 'accuracy']]\n",
    "us_postal_codes.columns = ['zip', 'city', 'state', 'latitude', 'longitude', 'accuracy']\n",
    "\n",
    "# Data type conversion\n",
    "us_postal_codes.zip = us_postal_codes.zip.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "839fedb2-5faa-4bf2-8f09-b8a44a14e159",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding Metro and City names (where present in the Zillow table)\n",
    "\n",
    "zillow_zhvi_df_slice_zip_unduplicated = zillow_zhvi_df_slice.drop_duplicates(subset=['zip'])\n",
    "us_postal_codes_with_names = pd.merge(us_postal_codes, zillow_zhvi_df_slice_zip_unduplicated[[ 'zip','metro', 'county']], on='zip', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "17f0e034-128f-41b4-821a-1fb039acff66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table:  zipcode_table saved.\n"
     ]
    }
   ],
   "source": [
    "# Saving final table as parquet file\n",
    "out_dir = 'data/etl_data/zipcode_table'\n",
    "save_pd_to_parquet(dtframe=us_postal_codes_with_names, fldr_name=out_dir, table_name='zipcode_table')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9202dba-3437-445b-b90a-1e794c1f6474",
   "metadata": {},
   "source": [
    "# Saving Data to AWS S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "581188a6-cac1-4b44-9e90-88cb60cb4413",
   "metadata": {},
   "outputs": [],
   "source": [
    "import configparser\n",
    "import boto3\n",
    "import awswrangler as wr\n",
    "\n",
    "aws_config = configparser.ConfigParser()\n",
    "aws_config.read('aws.cfg')\n",
    "ACCESS_ID = aws_config.get('AWS', 'AWS_ACCESS_KEY_ID')\n",
    "ACCESS_KEY = aws_config.get('AWS', 'AWS_SECRET_ACCESS_KEY')\n",
    "out_s3_dir = aws_config.get('AWS', 'OUTPUT_S3_BUCKET')\n",
    "\n",
    "my_session = boto3.Session(\n",
    "                            aws_access_key_id=ACCESS_ID,\n",
    "                            aws_secret_access_key=ACCESS_KEY,\n",
    "                            region_name=\"us-east-2\"\n",
    "                          )\n",
    "\n",
    "def save_parquet_to_s3(df, s3_bucket, folder_name, file_name, boto_session):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        df: dataframe to write\n",
    "        s3_bucket: destination s3_bucket\n",
    "        folder_name: name of the folder where the output parquet file will be saved.\n",
    "        file_name: name of the parquet file\n",
    "    \n",
    "    \"\"\"\n",
    "    out_path = s3_bucket + folder_name + '/' + file_name\n",
    "    wr.s3.to_parquet(\n",
    "                    df=df,\n",
    "                    path=out_path, \n",
    "                    boto3_session = boto_session,\n",
    "                )\n",
    "    return wr.s3.does_object_exist(out_path, boto3_session=boto_session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "53b75a00-9da2-4077-a683-5e12dd5645a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # Saving finals table to Amazon S3 as parquet file\n",
    "\n",
    "save_parquet_to_s3(df=zip_price_master_df.reset_index(), s3_bucket=out_s3_dir, folder_name='etl_data/zip_year_house_price_table', file_name='house_price_table.parquet', boto_session=my_session) # Seems there is some issues with writing multi-index df using awswrangler\n",
    "save_parquet_to_s3(df=us_postal_codes_with_names, s3_bucket=out_s3_dir, folder_name='etl_data/zipcode_table', file_name='zipcode_table.parquet', boto_session=my_session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "feb8a9bc-5032-4220-99a7-1ee7b106aa94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://aws-sdk-pandas.readthedocs.io/en/stable/stubs/awswrangler.s3.read_parquet.html\n",
    "\n",
    "house_price_s3_df = wr.s3.read_parquet(path=out_s3_dir + 'etl_data/zip_year_house_price_table/house_price_table.parquet', boto3_session=my_session, map_types=False)\n",
    "zip_s3_df = wr.s3.read_parquet(path=out_s3_dir + 'etl_data/zipcode_table/zipcode_table.parquet', boto3_session=my_session, map_types=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6b1a1972-2f55-4493-8b11-ef72a2799cdb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>VALUE_count_uad</th>\n",
       "      <th>VALUE_mean_uad</th>\n",
       "      <th>VALUE_median_uad</th>\n",
       "      <th>VALUE_p25_uad</th>\n",
       "      <th>VALUE_p75_uad</th>\n",
       "      <th>median_sale_price_redfin</th>\n",
       "      <th>median_list_price_redfin</th>\n",
       "      <th>median_ppsf_redfin</th>\n",
       "      <th>homes_sold_redfin</th>\n",
       "      <th>pending_sales_redfin</th>\n",
       "      <th>...</th>\n",
       "      <th>median_DOM_realtor</th>\n",
       "      <th>new_list_count_realtor</th>\n",
       "      <th>price_increase_count_realtor</th>\n",
       "      <th>price_reduced_count_realtor</th>\n",
       "      <th>pending_list_count_realtor</th>\n",
       "      <th>median_list_price_per_square_foot_realtor</th>\n",
       "      <th>median_square_feet_realtor</th>\n",
       "      <th>total_list_count_realtor</th>\n",
       "      <th>pending_ratio_realtor</th>\n",
       "      <th>zhvi_usd_dominated_zillow</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zip</th>\n",
       "      <th>year</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">501</th>\n",
       "      <th>2012</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>180000.0</td>\n",
       "      <td>269990.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>255000.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>246524.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>195.665236</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>206000.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>155367.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>96.024104</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>150.200000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>90.100000</td>\n",
       "      <td>1436.800000</td>\n",
       "      <td>14.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">99929</th>\n",
       "      <th>2018</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>241.500000</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>171.250000</td>\n",
       "      <td>1745.833333</td>\n",
       "      <td>45.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>105.333333</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>176.750000</td>\n",
       "      <td>1989.416667</td>\n",
       "      <td>65.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>108.000000</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>158.166667</td>\n",
       "      <td>2368.500000</td>\n",
       "      <td>56.0</td>\n",
       "      <td>0.253950</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>226.750000</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>117.500000</td>\n",
       "      <td>2204.166667</td>\n",
       "      <td>52.0</td>\n",
       "      <td>0.289863</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>179.818182</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>144.272727</td>\n",
       "      <td>1859.636364</td>\n",
       "      <td>53.0</td>\n",
       "      <td>0.166700</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>549176 rows Ã— 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            VALUE_count_uad  VALUE_mean_uad  VALUE_median_uad  VALUE_p25_uad  \\\n",
       "zip   year                                                                     \n",
       "501   2012              NaN             NaN               NaN            NaN   \n",
       "      2013              NaN             NaN               NaN            NaN   \n",
       "      2015              NaN             NaN               NaN            NaN   \n",
       "      2016              NaN             NaN               NaN            NaN   \n",
       "      2017              NaN             NaN               NaN            NaN   \n",
       "...                     ...             ...               ...            ...   \n",
       "99929 2018              NaN             NaN               NaN            NaN   \n",
       "      2019              NaN             NaN               NaN            NaN   \n",
       "      2020              NaN             NaN               NaN            NaN   \n",
       "      2021              NaN             NaN               NaN            NaN   \n",
       "      2022              NaN             NaN               NaN            NaN   \n",
       "\n",
       "            VALUE_p75_uad  median_sale_price_redfin  median_list_price_redfin  \\\n",
       "zip   year                                                                      \n",
       "501   2012            NaN                  180000.0                  269990.0   \n",
       "      2013            NaN                  255000.0                       NaN   \n",
       "      2015            NaN                  246524.0                       NaN   \n",
       "      2016            NaN                  206000.0                       NaN   \n",
       "      2017            NaN                  155367.0                       NaN   \n",
       "...                   ...                       ...                       ...   \n",
       "99929 2018            NaN                       NaN                       NaN   \n",
       "      2019            NaN                       NaN                       NaN   \n",
       "      2020            NaN                       NaN                       NaN   \n",
       "      2021            NaN                       NaN                       NaN   \n",
       "      2022            NaN                       NaN                       NaN   \n",
       "\n",
       "            median_ppsf_redfin  homes_sold_redfin  pending_sales_redfin  ...  \\\n",
       "zip   year                                                               ...   \n",
       "501   2012                 NaN                6.0                   2.0  ...   \n",
       "      2013                 NaN                6.0                   0.0  ...   \n",
       "      2015          195.665236               10.0                   0.0  ...   \n",
       "      2016                 NaN                2.0                   0.0  ...   \n",
       "      2017           96.024104                4.0                   0.0  ...   \n",
       "...                        ...                ...                   ...  ...   \n",
       "99929 2018                 NaN                NaN                   NaN  ...   \n",
       "      2019                 NaN                NaN                   NaN  ...   \n",
       "      2020                 NaN                NaN                   NaN  ...   \n",
       "      2021                 NaN                NaN                   NaN  ...   \n",
       "      2022                 NaN                NaN                   NaN  ...   \n",
       "\n",
       "            median_DOM_realtor  new_list_count_realtor  \\\n",
       "zip   year                                               \n",
       "501   2012                 NaN                     NaN   \n",
       "      2013                 NaN                     NaN   \n",
       "      2015                 NaN                     NaN   \n",
       "      2016                 NaN                     NaN   \n",
       "      2017          150.200000                     0.0   \n",
       "...                        ...                     ...   \n",
       "99929 2018          241.500000                     2.0   \n",
       "      2019          105.333333                     6.0   \n",
       "      2020          108.000000                     6.0   \n",
       "      2021          226.750000                     2.0   \n",
       "      2022          179.818182                     6.0   \n",
       "\n",
       "            price_increase_count_realtor  price_reduced_count_realtor  \\\n",
       "zip   year                                                              \n",
       "501   2012                           NaN                          NaN   \n",
       "      2013                           NaN                          NaN   \n",
       "      2015                           NaN                          NaN   \n",
       "      2016                           NaN                          NaN   \n",
       "      2017                           0.0                          0.0   \n",
       "...                                  ...                          ...   \n",
       "99929 2018                           0.0                          2.0   \n",
       "      2019                           0.0                          6.0   \n",
       "      2020                           0.0                          0.0   \n",
       "      2021                           0.0                          0.0   \n",
       "      2022                           0.0                          0.0   \n",
       "\n",
       "            pending_list_count_realtor  \\\n",
       "zip   year                               \n",
       "501   2012                         NaN   \n",
       "      2013                         NaN   \n",
       "      2015                         NaN   \n",
       "      2016                         NaN   \n",
       "      2017                         0.0   \n",
       "...                                ...   \n",
       "99929 2018                         0.0   \n",
       "      2019                         0.0   \n",
       "      2020                         2.0   \n",
       "      2021                         8.0   \n",
       "      2022                         1.0   \n",
       "\n",
       "            median_list_price_per_square_foot_realtor  \\\n",
       "zip   year                                              \n",
       "501   2012                                        NaN   \n",
       "      2013                                        NaN   \n",
       "      2015                                        NaN   \n",
       "      2016                                        NaN   \n",
       "      2017                                  90.100000   \n",
       "...                                               ...   \n",
       "99929 2018                                 171.250000   \n",
       "      2019                                 176.750000   \n",
       "      2020                                 158.166667   \n",
       "      2021                                 117.500000   \n",
       "      2022                                 144.272727   \n",
       "\n",
       "            median_square_feet_realtor  total_list_count_realtor  \\\n",
       "zip   year                                                         \n",
       "501   2012                         NaN                       NaN   \n",
       "      2013                         NaN                       NaN   \n",
       "      2015                         NaN                       NaN   \n",
       "      2016                         NaN                       NaN   \n",
       "      2017                 1436.800000                      14.0   \n",
       "...                                ...                       ...   \n",
       "99929 2018                 1745.833333                      45.0   \n",
       "      2019                 1989.416667                      65.0   \n",
       "      2020                 2368.500000                      56.0   \n",
       "      2021                 2204.166667                      52.0   \n",
       "      2022                 1859.636364                      53.0   \n",
       "\n",
       "            pending_ratio_realtor  zhvi_usd_dominated_zillow  \n",
       "zip   year                                                    \n",
       "501   2012                    NaN                        NaN  \n",
       "      2013                    NaN                        NaN  \n",
       "      2015                    NaN                        NaN  \n",
       "      2016                    NaN                        NaN  \n",
       "      2017                    NaN                        NaN  \n",
       "...                           ...                        ...  \n",
       "99929 2018                    NaN                        NaN  \n",
       "      2019                    NaN                        NaN  \n",
       "      2020               0.253950                        NaN  \n",
       "      2021               0.289863                        NaN  \n",
       "      2022               0.166700                        NaN  \n",
       "\n",
       "[549176 rows x 26 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "house_price_s3_df.set_index(['zip', 'year'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09df5ec4-2fd6-4945-9384-51f0caaa7f4f",
   "metadata": {},
   "source": [
    "# Saving Data to Redshift as tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8f19a9db-2771-435a-bacc-3f2fda59e5fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing configurations to access AWS resources.\n",
    "import configparser\n",
    "\n",
    "aws_config = configparser.ConfigParser()\n",
    "aws_config.read('aws.cfg')\n",
    "ACCESS_ID = aws_config.get('AWS', 'AWS_ACCESS_KEY_ID')\n",
    "ACCESS_KEY = aws_config.get('AWS', 'AWS_SECRET_ACCESS_KEY')\n",
    "out_s3_dir = aws_config.get('AWS', 'OUTPUT_S3_BUCKET')\n",
    "redshift_host = aws_config.get('REDSHIFT', 'HOST')\n",
    "redshift_database = aws_config.get('REDSHIFT', 'DATABASE')\n",
    "redshift_database_schema = aws_config.get('REDSHIFT', 'SCHEMA')\n",
    "redshift_user = aws_config.get('REDSHIFT', 'REDSHIFT_USER')\n",
    "redshift_password = aws_config.get('REDSHIFT', 'REDSHIFT_PASSWORD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3d0ad9b4-70d6-402b-a63d-18c5aeab41d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Redshift connector for awswrangler\n",
    "# https://docs.aws.amazon.com/redshift/latest/mgmt/python-connect-examples.html\n",
    "\n",
    "import redshift_connector\n",
    "conn = redshift_connector.connect(\n",
    "     host= redshift_host, #'examplecluster.abc123xyz789.us-west-1.redshift.amazonaws.com',\n",
    "     database= redshift_database, #  'dev',\n",
    "     user= redshift_user, #'awsuser',\n",
    "     password= redshift_password, #'my_password'\n",
    "  )\n",
    "\n",
    "cursor = conn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "6439d7b6-76ea-476b-b1e5-28a861612fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import awswrangler as wr\n",
    "\n",
    "# Copy zip code table to redshift database. Note: Data is staged in S3 as an intermediate step.\n",
    "wr.redshift.copy(df=us_postal_codes_with_names,\n",
    "                con=conn,\n",
    "                path= out_s3_dir + 'empty_dir/',\n",
    "                table='us_postal_code_table',\n",
    "                boto3_session=my_session,\n",
    "                schema = redshift_database_schema,\n",
    "                sortkey=['zip'],\n",
    "                mode='overwrite'\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "32754f5f-ff6a-4324-9987-fbd6bf6adbca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy zip house price table to redshift database. Note: Data is staged in S3 as an intermediate step.\n",
    "wr.redshift.copy(df=zip_price_master_df.reset_index(),\n",
    "                con=conn,\n",
    "                path= out_s3_dir + 'empty_dir/',\n",
    "                table='house_price_table',\n",
    "                boto3_session=my_session,\n",
    "                schema = redshift_database_schema,\n",
    "                sortkey=['zip'],\n",
    "                mode='overwrite'\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "153fc9f3-bac4-41b9-a8b2-8f8ae89a2e3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>zip</th>\n",
       "      <th>year</th>\n",
       "      <th>zhvi_usd_dominated_zillow</th>\n",
       "      <th>city</th>\n",
       "      <th>metro</th>\n",
       "      <th>county</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>77030</td>\n",
       "      <td>2000</td>\n",
       "      <td>254395.166667</td>\n",
       "      <td>Houston</td>\n",
       "      <td>Houston-The Woodlands-Sugar Land, TX</td>\n",
       "      <td>Harris County</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>77030</td>\n",
       "      <td>2001</td>\n",
       "      <td>258776.083333</td>\n",
       "      <td>Houston</td>\n",
       "      <td>Houston-The Woodlands-Sugar Land, TX</td>\n",
       "      <td>Harris County</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>77030</td>\n",
       "      <td>2002</td>\n",
       "      <td>268662.166667</td>\n",
       "      <td>Houston</td>\n",
       "      <td>Houston-The Woodlands-Sugar Land, TX</td>\n",
       "      <td>Harris County</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>77030</td>\n",
       "      <td>2003</td>\n",
       "      <td>270433.666667</td>\n",
       "      <td>Houston</td>\n",
       "      <td>Houston-The Woodlands-Sugar Land, TX</td>\n",
       "      <td>Harris County</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>77030</td>\n",
       "      <td>2004</td>\n",
       "      <td>279167.000000</td>\n",
       "      <td>Houston</td>\n",
       "      <td>Houston-The Woodlands-Sugar Land, TX</td>\n",
       "      <td>Harris County</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>77030</td>\n",
       "      <td>2005</td>\n",
       "      <td>292961.083333</td>\n",
       "      <td>Houston</td>\n",
       "      <td>Houston-The Woodlands-Sugar Land, TX</td>\n",
       "      <td>Harris County</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>77030</td>\n",
       "      <td>2006</td>\n",
       "      <td>302658.333333</td>\n",
       "      <td>Houston</td>\n",
       "      <td>Houston-The Woodlands-Sugar Land, TX</td>\n",
       "      <td>Harris County</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>77030</td>\n",
       "      <td>2007</td>\n",
       "      <td>308546.916667</td>\n",
       "      <td>Houston</td>\n",
       "      <td>Houston-The Woodlands-Sugar Land, TX</td>\n",
       "      <td>Harris County</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>77030</td>\n",
       "      <td>2008</td>\n",
       "      <td>308866.166667</td>\n",
       "      <td>Houston</td>\n",
       "      <td>Houston-The Woodlands-Sugar Land, TX</td>\n",
       "      <td>Harris County</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>77030</td>\n",
       "      <td>2009</td>\n",
       "      <td>315686.750000</td>\n",
       "      <td>Houston</td>\n",
       "      <td>Houston-The Woodlands-Sugar Land, TX</td>\n",
       "      <td>Harris County</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>77030</td>\n",
       "      <td>2010</td>\n",
       "      <td>315835.583333</td>\n",
       "      <td>Houston</td>\n",
       "      <td>Houston-The Woodlands-Sugar Land, TX</td>\n",
       "      <td>Harris County</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>77030</td>\n",
       "      <td>2011</td>\n",
       "      <td>306035.416667</td>\n",
       "      <td>Houston</td>\n",
       "      <td>Houston-The Woodlands-Sugar Land, TX</td>\n",
       "      <td>Harris County</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>77030</td>\n",
       "      <td>2012</td>\n",
       "      <td>310758.166667</td>\n",
       "      <td>Houston</td>\n",
       "      <td>Houston-The Woodlands-Sugar Land, TX</td>\n",
       "      <td>Harris County</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>77030</td>\n",
       "      <td>2013</td>\n",
       "      <td>344890.333333</td>\n",
       "      <td>Houston</td>\n",
       "      <td>Houston-The Woodlands-Sugar Land, TX</td>\n",
       "      <td>Harris County</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>77030</td>\n",
       "      <td>2014</td>\n",
       "      <td>386409.250000</td>\n",
       "      <td>Houston</td>\n",
       "      <td>Houston-The Woodlands-Sugar Land, TX</td>\n",
       "      <td>Harris County</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>77030</td>\n",
       "      <td>2015</td>\n",
       "      <td>424351.833333</td>\n",
       "      <td>Houston</td>\n",
       "      <td>Houston-The Woodlands-Sugar Land, TX</td>\n",
       "      <td>Harris County</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>77030</td>\n",
       "      <td>2016</td>\n",
       "      <td>428553.583333</td>\n",
       "      <td>Houston</td>\n",
       "      <td>Houston-The Woodlands-Sugar Land, TX</td>\n",
       "      <td>Harris County</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>77030</td>\n",
       "      <td>2017</td>\n",
       "      <td>432786.666667</td>\n",
       "      <td>Houston</td>\n",
       "      <td>Houston-The Woodlands-Sugar Land, TX</td>\n",
       "      <td>Harris County</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>77030</td>\n",
       "      <td>2018</td>\n",
       "      <td>435205.083333</td>\n",
       "      <td>Houston</td>\n",
       "      <td>Houston-The Woodlands-Sugar Land, TX</td>\n",
       "      <td>Harris County</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>77030</td>\n",
       "      <td>2019</td>\n",
       "      <td>436612.250000</td>\n",
       "      <td>Houston</td>\n",
       "      <td>Houston-The Woodlands-Sugar Land, TX</td>\n",
       "      <td>Harris County</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>77030</td>\n",
       "      <td>2020</td>\n",
       "      <td>434031.000000</td>\n",
       "      <td>Houston</td>\n",
       "      <td>Houston-The Woodlands-Sugar Land, TX</td>\n",
       "      <td>Harris County</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>77030</td>\n",
       "      <td>2021</td>\n",
       "      <td>459254.083333</td>\n",
       "      <td>Houston</td>\n",
       "      <td>Houston-The Woodlands-Sugar Land, TX</td>\n",
       "      <td>Harris County</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>77030</td>\n",
       "      <td>2022</td>\n",
       "      <td>493039.181818</td>\n",
       "      <td>Houston</td>\n",
       "      <td>Houston-The Woodlands-Sugar Land, TX</td>\n",
       "      <td>Harris County</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      zip  year  zhvi_usd_dominated_zillow     city  \\\n",
       "0   77030  2000              254395.166667  Houston   \n",
       "1   77030  2001              258776.083333  Houston   \n",
       "2   77030  2002              268662.166667  Houston   \n",
       "3   77030  2003              270433.666667  Houston   \n",
       "4   77030  2004              279167.000000  Houston   \n",
       "5   77030  2005              292961.083333  Houston   \n",
       "6   77030  2006              302658.333333  Houston   \n",
       "7   77030  2007              308546.916667  Houston   \n",
       "8   77030  2008              308866.166667  Houston   \n",
       "9   77030  2009              315686.750000  Houston   \n",
       "10  77030  2010              315835.583333  Houston   \n",
       "11  77030  2011              306035.416667  Houston   \n",
       "12  77030  2012              310758.166667  Houston   \n",
       "13  77030  2013              344890.333333  Houston   \n",
       "14  77030  2014              386409.250000  Houston   \n",
       "15  77030  2015              424351.833333  Houston   \n",
       "16  77030  2016              428553.583333  Houston   \n",
       "17  77030  2017              432786.666667  Houston   \n",
       "18  77030  2018              435205.083333  Houston   \n",
       "19  77030  2019              436612.250000  Houston   \n",
       "20  77030  2020              434031.000000  Houston   \n",
       "21  77030  2021              459254.083333  Houston   \n",
       "22  77030  2022              493039.181818  Houston   \n",
       "\n",
       "                                   metro         county  \n",
       "0   Houston-The Woodlands-Sugar Land, TX  Harris County  \n",
       "1   Houston-The Woodlands-Sugar Land, TX  Harris County  \n",
       "2   Houston-The Woodlands-Sugar Land, TX  Harris County  \n",
       "3   Houston-The Woodlands-Sugar Land, TX  Harris County  \n",
       "4   Houston-The Woodlands-Sugar Land, TX  Harris County  \n",
       "5   Houston-The Woodlands-Sugar Land, TX  Harris County  \n",
       "6   Houston-The Woodlands-Sugar Land, TX  Harris County  \n",
       "7   Houston-The Woodlands-Sugar Land, TX  Harris County  \n",
       "8   Houston-The Woodlands-Sugar Land, TX  Harris County  \n",
       "9   Houston-The Woodlands-Sugar Land, TX  Harris County  \n",
       "10  Houston-The Woodlands-Sugar Land, TX  Harris County  \n",
       "11  Houston-The Woodlands-Sugar Land, TX  Harris County  \n",
       "12  Houston-The Woodlands-Sugar Land, TX  Harris County  \n",
       "13  Houston-The Woodlands-Sugar Land, TX  Harris County  \n",
       "14  Houston-The Woodlands-Sugar Land, TX  Harris County  \n",
       "15  Houston-The Woodlands-Sugar Land, TX  Harris County  \n",
       "16  Houston-The Woodlands-Sugar Land, TX  Harris County  \n",
       "17  Houston-The Woodlands-Sugar Land, TX  Harris County  \n",
       "18  Houston-The Woodlands-Sugar Land, TX  Harris County  \n",
       "19  Houston-The Woodlands-Sugar Land, TX  Harris County  \n",
       "20  Houston-The Woodlands-Sugar Land, TX  Harris County  \n",
       "21  Houston-The Woodlands-Sugar Land, TX  Harris County  \n",
       "22  Houston-The Woodlands-Sugar Land, TX  Harris County  "
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check redshift database\n",
    "\n",
    "sql_query = \"\"\"SELECT l.zip , l.year, l.zhvi_usd_dominated_zillow, r.city, r.metro, r.county\n",
    "FROM (select zip, year, zhvi_usd_dominated_zillow from house_price_table where zip= 77030) as l left join us_postal_code_table r\n",
    "on l.zip = r.zip ORDER BY year\"\"\"\n",
    "\n",
    "wr.redshift.read_sql_query(\n",
    "                            sql = sql_query, #'SELECT count(*) from house_price_table limit 15', \n",
    "                            con= conn\n",
    "                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "ef37a5ae-54bd-4731-bed9-b38358e6d2cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9b79018-87e0-4f8c-acd4-e7e7ab430b97",
   "metadata": {},
   "source": [
    "# Saved file validity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "eac873eb-b0fe-43ac-ac12-666d9ae4cd09",
   "metadata": {},
   "outputs": [],
   "source": [
    "house_price_etl_data_path = 'data/etl_data/zip_year_house_price_table/house_price_table/'\n",
    "zipcode_etl_data_path = 'data/etl_data/zipcode_table/zipcode_table/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "0c4d8f81-8b3d-46d7-8dce-f9d1054974fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_validity_check(data_path):\n",
    "    import pandas as pd\n",
    "    try:\n",
    "        dataframe = pd.read_parquet(path=data_path)\n",
    "    except:\n",
    "        raise FileNotFoundError(f'File not present at {data_path}')\n",
    "    df_length = dataframe.shape[0]\n",
    "    if df_length == 0:\n",
    "        raise ValueError('Dataframe seems to be empty! .shape[0] gave length of 0!!!')\n",
    "    elif (dataframe.isnull().sum() == df_length).sum() >0 :\n",
    "        raise ValueError('At least one of the columns have all Null values.')\n",
    "    else:\n",
    "        return print(f\"No errors raised for parquet data at {data_path}. \\nLooks good :)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "f0d27f38-ad7b-4c91-91e6-684d264cf323",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No errors raised for parquet data at data/etl_data/zip_year_house_price_table/house_price_table/. \n",
      "Looks good :)\n"
     ]
    }
   ],
   "source": [
    "data_validity_check(house_price_etl_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "fb6be130-95a6-40cf-bbbb-968ae4eae405",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No errors raised for parquet data at data/etl_data/zipcode_table/zipcode_table/. \n",
      "Looks good :)\n"
     ]
    }
   ],
   "source": [
    "data_validity_check(zipcode_etl_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f43ccdf0-dc25-4e7b-a89f-0b697f06c3d5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
