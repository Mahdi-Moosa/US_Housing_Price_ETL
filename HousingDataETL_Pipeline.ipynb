{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "15f590af-91f8-4dfa-b59a-37f02458aa30",
   "metadata": {
    "tags": []
   },
   "source": [
    "# UAD (Uniform Appraisal Data) Data ETL\n",
    "\n",
    "Federal Housing Finance Agency (FHFA) releases housing appraisal dataset (anually/quarterly). This dataset is aggregated from home value appraisals for refinance and/or purchase. UAD dataset can provide insights into local appraised house prices.\n",
    "\n",
    "Obviously there are some caveats to using UAD data as a represntative for house prices. See the following link:\n",
    "\n",
    "https://www.fhfa.gov/Media/Blog/Pages/Exploring-Appraisal-Bias-Using-UAD-Aggregate-Statistics.aspx\n",
    "\n",
    "**Notes on the raw data**\n",
    "\n",
    "* UAD Data is aggregated by census tract.\n",
    "* There were some updates in census tract 2010 vs. census tract 2020.\n",
    "* Mapping between zip code and census tract is not one-to-one.\n",
    "\n",
    "## Based on the aforementioned facts, following steps were performed to get approximate home appraisal values for each zip codes:\n",
    "\n",
    "* UAD dataset gives data based on census tract 2020 census tract.\n",
    "* HUD provides map from census tract 2010 to US zip codes.\n",
    "* The relationship between census tract 2010 to census tract 2020 is provided by census.gov.\n",
    "\n",
    "* We will map (UAD dataset census tract 2020) to (census tract 2010) to (USPS Zip Code). ***These mappings are not one-to-one. To simplify our analysis, we only keep the first x-to-y map and drop other x-to-y1, x-to-y2 maps, essentailly making the relationship one-to-one. Our justification is - cenusus tracts will map to nearby zip codes and therefore, even if we are wrong in our x-to-y mapping, the close vicinity of y1, y2 still allows us to approximate localized home value appraisal.***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5106c49a-50e0-40a3-96df-d6676f0c3414",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Downloading relevant data files (if not present)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ab4818b5-81cc-4993-b8b7-4944ab410068",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to check for data file(s) and fetch data them, if not present.\n",
    "\n",
    "import requests, os, time\n",
    "\n",
    "def fetch_file(url, folder_name, presence_check=True,  fname=None):\n",
    "    \"\"\" Fetches file from URL.\n",
    "    Input:\n",
    "      fname: File name to save If not provided, file name is derived from the provided URL. String part after the last '/' in the url.\n",
    "      url: Url to download.\n",
    "      presence_check: When true - Will check for the file's presence and if present, file won't be downloaded. Default value is true. If you want to overwrite existing value, please pass False to this parameter.\n",
    "    \n",
    "    Return:\n",
    "      Saves file at the specified filepath. When presence_check is True and file is already present, prints \"File already present.\" statement.\n",
    "    \"\"\"  \n",
    "    if fname is None:\n",
    "        fname = url.split('/')[-1]\n",
    "        fname = fname.split('?')[0]\n",
    "        \n",
    "    fpath = folder_name + '/' + fname\n",
    "\n",
    "    if presence_check and os.path.exists(f'{fpath}'): #TODO: File size check validation.\n",
    "        return print('File already present.')\n",
    "\n",
    "    if not os.path.isdir(folder_name):\n",
    "        os.makedirs(folder_name)\n",
    "\n",
    "    response = requests.get(url, stream=True)\n",
    "    time.sleep(2)\n",
    "    total_length = response.headers.get('content-length')\n",
    "    total_length = round(int(total_length)/1e6,2)\n",
    "    \n",
    "    print(f'To be saved as {fpath}. Total size to be written is: {total_length} MB') \n",
    "\n",
    "    with open(fpath, \"wb\") as f:\n",
    "        for chunk in response.iter_content(chunk_size=512):\n",
    "            if chunk:  # filter out keep-alive new chunks\n",
    "                f.write(chunk)\n",
    "    \n",
    "    if os.path.exists(f'{fpath}'): #TODO: File size check validation.\n",
    "        return print(f'File download succesful for {fname}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a183f6fa-f501-4c8c-bfd2-0882473ca270",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get UAD urls to download\n",
    "\n",
    "import configparser\n",
    "\n",
    "config = configparser.ConfigParser()\n",
    "config.read('hpa.cfg')\n",
    "\n",
    "uad_url = config.get('UAD', 'uad_url')\n",
    "tract_2_zip_url = config.get('UAD', 'tract_2_zip_url')\n",
    "tract20_10_map_url = config.get('UAD', 'tract20_10_map_url')\n",
    "zip_url = config.get('UAD', 'zip_url')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1b6c66ca-fe30-4434-96d1-a2eceae0f055",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already present.\n",
      "File already present.\n",
      "File already present.\n",
      "File already present.\n"
     ]
    }
   ],
   "source": [
    "# Download URLs \n",
    "fetch_file(folder_name='data/raw_data/FHFA-UAD',url=uad_url)\n",
    "fetch_file(folder_name='data/raw_data/HUD-USPS', url=tract_2_zip_url)\n",
    "fetch_file(folder_name='data/raw_data/census', url=tract20_10_map_url)\n",
    "fetch_file(folder_name='data/raw_data/geonames', url=zip_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b747023-6e68-4cd0-bea5-e32cb5e8803a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Loading Files - Data Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "19bbdd0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UAD data loaded successfully.\n",
      "Mapping data from tract_2010 to zip code loaded.\n",
      "Census tract 2020 to census tract 2010 map loaded.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load UAD data, drop rows with Null appriasal values in the VALUE column.\n",
    "uad_df = pd.read_csv('data/raw_data/FHFA-UAD/UADAggs_tract.zip')\n",
    "print('UAD data loaded successfully.')\n",
    "uad_df = uad_df[uad_df.VALUE.notna()]\n",
    "\n",
    "# Load tract to zip code data, drop one-to-many relationships\n",
    "tract_2_zip = pd.read_excel('data/raw_data/HUD-USPS/TRACT_ZIP_122021.xlsx')\n",
    "tract_2_zip = tract_2_zip.drop_duplicates(subset='tract')\n",
    "print('Mapping data from tract_2010 to zip code loaded.')\n",
    "\n",
    "# Load tract 2010 to tract 2020 map, select relevant colums and drop one-to-many relationships\n",
    "tract_2010_to_tract_2020 = pd.read_csv('data/raw_data/census/tab20_tract20_tract10_natl.txt', sep='|')\n",
    "print('Census tract 2020 to census tract 2010 map loaded.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "950724e1-ef5d-4cde-a62b-5fdda22c78ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data selection, drop duplicates\n",
    "tract_2020_2010_map = tract_2010_to_tract_2020[['GEOID_TRACT_20','GEOID_TRACT_10']]\n",
    "tract_2020_2010_map = tract_2020_2010_map.drop_duplicates(subset='GEOID_TRACT_20')\n",
    "\n",
    "# Maping census tract 2020 to census tract 2010.\n",
    "uad_df = uad_df.merge(tract_2020_2010_map, left_on='TRACT', right_on='GEOID_TRACT_20', how='left')\n",
    "\n",
    "# Dropping Null entries on GEOID_TRACT_20, if any\n",
    "uad_df = uad_df[uad_df.GEOID_TRACT_20.notna()]\n",
    "\n",
    "# Column data type assignment\n",
    "\n",
    "uad_df.GEOID_TRACT_10 = uad_df.GEOID_TRACT_10.astype('int64')\n",
    "uad_df.GEOID_TRACT_20 = uad_df.GEOID_TRACT_20.astype('int64')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0d07ba0-3493-46ea-bf63-8a2b20562aef",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Census Track to zip map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "82e0ca52-da46-44aa-97c9-e7c51500de35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapping Zip to Census tract 2010\n",
    "uad_df_merged = uad_df.merge(tract_2_zip[['tract', 'zip']], left_on = 'GEOID_TRACT_10', right_on='tract', how='left')\n",
    "\n",
    "# Dropping null Zip entries.\n",
    "uad_df_merged = uad_df_merged[uad_df_merged.zip.notna()].copy(deep=True)\n",
    "\n",
    "# Data type conversion\n",
    "uad_df_merged.zip = uad_df_merged.zip.astype(int)\n",
    "\n",
    "# Dropping dupllicate columns\n",
    "uad_df_merged.drop(columns='tract', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ad18224-3d08-4917-a349-20b6958b9a15",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Exploration Final UAD DF Merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8a7f8cd4-1581-483b-ad6b-fd556fc89195",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Both         615689\n",
       "Purchase     459814\n",
       "Refinance    459743\n",
       "Name: PURPOSE, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uad_df_merged[uad_df_merged.SERIESID == 'MEAN'].PURPOSE.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a8bdcba-44b4-4293-a800-df6c90298148",
   "metadata": {},
   "source": [
    "As per documentation, Purchase + Refinance = Both.\n",
    "\n",
    "However, it appears that there is an imbalance. I have found some entries with only *Both* values.\n",
    "Perhaps this is due to:\n",
    "* Record suppression - to make records anonymous\n",
    "* Incomplete data - data does not distinguish between Purchase/Refinance or suggest both!\n",
    "\n",
    "**For our puroposes, we should get both only. At least, initially!**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a2ead34-e5d4-48f0-954e-5c245e78ebde",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Selecting relevant columns, and save final merged UAD table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f05a00fe-7b9c-4c65-9a9a-06d5d3438763",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting only relevant columns\n",
    "uad_df_merged_slice_to_save =  uad_df_merged[['SERIESID', 'PURPOSE', 'YEAR', 'VALUE', 'GEOID_TRACT_10', 'GEOID_TRACT_20', 'zip']]\n",
    "\n",
    "# Filtering dataframe based on relevant SERIESIDs.\n",
    "uad_df_merged_slice_to_save = uad_df_merged_slice_to_save[uad_df_merged_slice_to_save.SERIESID.isin(['COUNT', 'MEDIAN', 'P25', 'P75', 'MEAN'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "569d521d-d6ee-44d6-8825-aac357d09959",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting UAD table into count, mean, median, p25 and p75 tables\n",
    "count_table = uad_df_merged_slice_to_save[(uad_df_merged_slice_to_save.PURPOSE == 'Both')\n",
    "                           & (uad_df_merged_slice_to_save.SERIESID == 'COUNT')]\n",
    "\n",
    "mean_table = uad_df_merged_slice_to_save[(uad_df_merged_slice_to_save.PURPOSE == 'Both')\n",
    "                           & (uad_df_merged_slice_to_save.SERIESID == 'MEAN')]\n",
    "\n",
    "median_table = uad_df_merged_slice_to_save[(uad_df_merged_slice_to_save.PURPOSE == 'Both')\n",
    "                           & (uad_df_merged_slice_to_save.SERIESID == 'MEDIAN')]\n",
    "\n",
    "p25_table = uad_df_merged_slice_to_save[(uad_df_merged_slice_to_save.PURPOSE == 'Both')\n",
    "                           & (uad_df_merged_slice_to_save.SERIESID == 'P25')]\n",
    "\n",
    "p75_table = uad_df_merged_slice_to_save[(uad_df_merged_slice_to_save.PURPOSE == 'Both')\n",
    "                           & (uad_df_merged_slice_to_save.SERIESID == 'P75')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "67b0f6d1-916c-4baf-9b11-a00209fe4d2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.26590535156548195"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking table statistics\n",
    "count_table.drop_duplicates(subset=['zip', 'YEAR']).shape[0]/count_table.drop_duplicates(subset=['GEOID_TRACT_20', 'YEAR']).shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f613fc65-7be5-4903-a5b7-9355c36151ce",
   "metadata": {},
   "source": [
    "Conclusion: ~75% of the zip codes repeated. Perhaps, this is due to the drop_duplicates method applied in the track_to_zip dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dd564d44-0d00-4715-928e-937935aa0dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Performing appropriate aggregation operation on count, mean, median, p25 and p75 tables. \n",
    "\n",
    "zip_count_table = count_table[['zip', 'YEAR', 'VALUE']].groupby(['zip', 'YEAR']).sum('VALUE').rename(columns={'VALUE':'VALUE_count'})\n",
    "zip_mean_table = mean_table[['zip', 'YEAR', 'VALUE']].groupby(['zip', 'YEAR']).mean('VALUE').rename(columns={'VALUE':'VALUE_mean'})\n",
    "zip_median_table = median_table[['zip', 'YEAR', 'VALUE']].groupby(['zip', 'YEAR']).mean('VALUE').rename(columns={'VALUE':'VALUE_median'})\n",
    "zip_p25_table = p25_table[['zip', 'YEAR', 'VALUE']].groupby(['zip', 'YEAR']).mean('VALUE').rename(columns={'VALUE':'VALUE_p25'})\n",
    "zip_p75_table = p75_table[['zip', 'YEAR', 'VALUE']].groupby(['zip', 'YEAR']).mean('VALUE').rename(columns={'VALUE':'VALUE_p75'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c495b6f2-e3a0-4800-a3a5-454e95d41834",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combining zip count, mean, median, p25 and p75 tables into single table where UAD data is turned into single table. This table is the end table of the ETL process for UAD data\n",
    "from functools import reduce\n",
    "\n",
    "dfs_to_merge = [zip_count_table, zip_mean_table, zip_median_table, zip_p25_table, zip_p75_table]\n",
    "\n",
    "zip_uad_df_merged = reduce(lambda  left,right: pd.merge(left,right,left_index=True, right_index=True,\n",
    "                                            how='left'), dfs_to_merge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "994310c0-02e8-4d4d-9a72-b8bd934be3e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save final UAD data table\n",
    "\n",
    "def save_pd_to_parquet(dtframe, fldr_name, table_name):\n",
    "    \"\"\"\n",
    "    This function saves dataframe as parquet file at specified folder locations. \n",
    "    Input:\n",
    "        fldr_name: Folder name where data will be saved. Sub-directory supported. For example, you can specificy \"destination_folder\" or you can specify \"destination_folder/yet_another_folder\".\n",
    "        table_name: This is the table name for parquet file(s). Data will be saved in a subdirectory under specified fldr_name with actual .parquet file with a timestamp. For example, \n",
    "            if table name is specified as \"abc\" then the folder organization will be \n",
    "                        | - fldr_name\n",
    "                        | -- abc\n",
    "                        | ---- abc_{os.timestamp}.parquet\n",
    "        dtframe: Input dataframe.\n",
    "    Return(s):\n",
    "        print statement saying data write was sucessful.\n",
    "    \"\"\"\n",
    "    import os\n",
    "    from datetime import datetime\n",
    "    cur_time = str(datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\"))\n",
    "    file_path = fldr_name + '/' + table_name\n",
    "    if not os.path.isdir(file_path):\n",
    "        os.makedirs(file_path)\n",
    "    dtframe.to_parquet(path=f'{fldr_name}/{table_name}/{table_name}_{cur_time}.parquet')\n",
    "    return print(f'Table:  {table_name} saved.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6273dc6e-c31f-4d31-8436-ce7323cd03e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table:  zip_uad_table saved.\n"
     ]
    }
   ],
   "source": [
    "# Save file to specified directory\n",
    "\n",
    "out_dir = 'data/etl_data/uad_appraisal'\n",
    "save_pd_to_parquet(dtframe=zip_uad_df_merged, fldr_name=out_dir, table_name='zip_uad_table')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d65f415-563c-47ef-9c65-26e686f8f55f",
   "metadata": {},
   "source": [
    "**NOTE**: Caveats on census tract to zip code mapping.\n",
    "\n",
    "The relationship between census tract and zip code is not often one-to-one. Here though, for our purposes, we approximated the relationship to be one-to-one (and dropped *duplicated* relations). Justifications:\n",
    "\n",
    "* We are interested in local prices. For one-to-many census tract to zip code relations, multiple zip codes that map to the same census tract are adjacent. For our purposes, mis-assignment of a zip code to its neigbouring one - while not ideal - is not detrimental."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c6dbaeb-3e6e-4f83-b4c5-b01d8f8aecb2",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Realtor Data\n",
    "\n",
    "Realtor.com provides real estate data *monthly/weekly* at https://www.realtor.com/research/data/.\n",
    "\n",
    "Here, we are retrieving the monthly data and convert that into yearly data for each of the zip codes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "611dfb78-5bf2-4521-b48c-96626661b391",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Download Realtor Data (if not present already)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "32f87eb3-1e13-4727-81b9-454cf4530eb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already present.\n"
     ]
    }
   ],
   "source": [
    "realtor_url = config.get('REALTOR', 'realtor_url')\n",
    "\n",
    "fetch_file(url=realtor_url, folder_name='data/raw_data/realtor_data/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ade69936-47da-417a-a653-e9485ca2fa31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Realtor.com data successfully read from the .csv file.\n"
     ]
    }
   ],
   "source": [
    "# Reading data from downoaded zip file\n",
    "realtor_df = pd.read_csv('data/raw_data/realtor_data/RDC_Inventory_Core_Metrics_Zip_History.csv', low_memory=False)\n",
    "print('Realtor.com data successfully read from the .csv file.')\n",
    "\n",
    "realtor_df = realtor_df.iloc[0:-1] # Dropping last line that contains aggregated summary (line Total)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "766c029c-f79b-486a-9b3b-8cd7851dc40c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting relevant columns.\n",
    "\n",
    "realtor_cols = ['month_date_yyyymm', 'postal_code', 'median_listing_price', 'average_listing_price', 'active_listing_count', 'median_days_on_market', 'new_listing_count', \n",
    "                'price_increased_count', 'price_reduced_count', 'pending_listing_count', 'median_listing_price_per_square_foot', 'median_square_feet', 'total_listing_count', \n",
    "               'pending_ratio', 'quality_flag']\n",
    "\n",
    "realtor_df_slice = realtor_df[realtor_cols].copy(deep=True)\n",
    "\n",
    "# Data format conversion\n",
    "realtor_df_slice.month_date_yyyymm = pd.to_datetime(realtor_df_slice.month_date_yyyymm, format='%Y%m') #.month_date_yyyymm.astype('datetime64[ns]')\n",
    "realtor_df_slice['postal_code'] = realtor_df_slice.postal_code.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a053d4e3-8243-4a1d-8481-f205ec7d7e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zip-Year aggregation\n",
    "\n",
    "realtor_df_slice_agg = realtor_df_slice.groupby(by=['postal_code', realtor_df_slice.month_date_yyyymm.dt.year]).agg(\n",
    "                                                                                            median_list_price = ('median_listing_price', 'mean'),\n",
    "                                                                                            avg_list_price = ('average_listing_price', 'mean'),\n",
    "                                                                                            active_list_count = ('active_listing_count', 'sum'),\n",
    "                                                                                            median_DOM = ('median_days_on_market', 'mean'),\n",
    "                                                                                            new_list_count = ('new_listing_count', 'sum'),\n",
    "                                                                                            price_increase_count = ('price_increased_count', 'sum'),\n",
    "                                                                                            price_reduced_count = ('price_reduced_count', 'sum'),\n",
    "                                                                                            pending_list_count = ('pending_listing_count', 'sum'),\n",
    "                                                                                            median_list_price_per_square_foot = ('median_listing_price_per_square_foot', 'mean'),\n",
    "                                                                                            median_square_feet = ('median_square_feet', 'mean'),\n",
    "                                                                                            total_list_count = ('total_listing_count', 'sum'),\n",
    "                                                                                            pending_ratio = ('pending_ratio', 'mean')\n",
    "                                                                                            )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f1e11a6-04de-4718-ac0e-2ccbe6684583",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Redfin data\n",
    "\n",
    "Redfin also releases data on housing market at https://www.redfin.com/news/data-center/."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4eef6538-0c94-4a61-89c7-4c27b9f49516",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already present.\n"
     ]
    }
   ],
   "source": [
    "import configparser\n",
    "\n",
    "config = configparser.ConfigParser()\n",
    "config.read('hpa.cfg')\n",
    "\n",
    "# Download redfin data\n",
    "\n",
    "redfin_url = config.get('REDFIN', 'redfin_url')\n",
    "\n",
    "fetch_file(url=redfin_url, folder_name='data/raw_data/redfin_data/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2850fbe0-fd09-49ab-815a-cee5e68d9fba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading redfin data from downloaded compressed file.\n",
    "redfin_df = pd.read_csv('data/raw_data/redfin_data/zip_code_market_tracker.tsv000.gz', sep='\\t')\n",
    "\n",
    "# Choosing relevant columns\n",
    "column_subset = ['period_end', 'property_type', 'median_sale_price', 'median_list_price', \n",
    "                'median_ppsf', 'homes_sold', 'pending_sales', 'new_listings',  'inventory',\n",
    "                'avg_sale_to_list', 'region' ]\n",
    "redfin_df_slice = redfin_df[column_subset].copy(deep=True)\n",
    "\n",
    "# Data reformating/ type conversion\n",
    "redfin_df_slice['zip'] = redfin_df_slice.region.str.split(': ', expand=True)[1].astype('int')\n",
    "redfin_df_slice.period_end = pd.to_datetime(redfin_df_slice.period_end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "517b19b5-f6aa-4570-9208-b90010cf033e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregating data on zip code, year.\n",
    "redfin_df_slice_zip_agg = redfin_df_slice.groupby(by=['zip', redfin_df_slice.period_end.dt.year]).agg(  median_sale_price = ('median_sale_price', 'mean'),\n",
    "                                                                                                        median_list_price = ('median_list_price', 'mean'),\n",
    "                                                                                                        median_ppsf = ('median_ppsf', 'mean'),\n",
    "                                                                                                        homes_sold = ('homes_sold', 'sum'),\n",
    "                                                                                                        pending_sales = ('pending_sales', 'sum'),\n",
    "                                                                                                        new_listings = ('new_listings', 'sum'),\n",
    "                                                                                                        inventory = ('inventory', 'sum'),\n",
    "                                                                                                        avg_sale_to_list = ('avg_sale_to_list', 'mean')\n",
    "                                                                                                     )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e34b0d2f-fb6f-4865-b9da-f80c400d306e",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Zwillow Data\n",
    "\n",
    "Zwillow releases research data at https://www.zillow.com/research/data/.\n",
    "\n",
    "Zwillow House Value Index (dollar-dominated) seemed the most comprehsive for zip-code-based dataset. If one is only interested in metro areas, they have more options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "418856a4-0146-4d08-b7ca-8c6deb9c4c69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already present.\n"
     ]
    }
   ],
   "source": [
    "import configparser\n",
    "\n",
    "config = configparser.ConfigParser()\n",
    "config.read('hpa.cfg')\n",
    "\n",
    "# Get zwillow data\n",
    "zwillow_url = config.get('ZWILLOW', 'zwillow_url')\n",
    "\n",
    "fetch_file(url=zwillow_url, folder_name='data/raw_data/zwillow_data/', fname='Zip_zhvi_uc_sfrcondo_tier_0.33_0.67_sm_sa_month.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f5bb304a-af49-48ab-92a8-2d75a0a8197e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zwillow data read succesful.\n"
     ]
    }
   ],
   "source": [
    "# Reading Zwillow Data from downloaded file\n",
    "import pandas as pd\n",
    "zwillow_zhvi_df = pd.read_csv('data/raw_data/zwillow_data/Zip_zhvi_uc_sfrcondo_tier_0.33_0.67_sm_sa_month.csv')\n",
    "print(\"Zwillow data read succesful.\")\n",
    "\n",
    "# Selecting columns of interest\n",
    "excluded_cols = ['RegionID', 'SizeRank','StateName', 'State','RegionType' ]\n",
    "zwillow_zhvi_df_slice = zwillow_zhvi_df[[x for x in zwillow_zhvi_df.columns if x not in excluded_cols]].copy(deep=True)\n",
    "zwillow_zhvi_df_slice = zwillow_zhvi_df_slice.set_index(['RegionName', 'City', 'Metro', 'CountyName']).stack().reset_index()\n",
    "zwillow_zhvi_df_slice.columns = ['zip', 'city', 'metro', 'county', 'date', 'zhvi_usd_dominated']\n",
    "\n",
    "# Data type conversion\n",
    "zwillow_zhvi_df_slice.date = pd.to_datetime(zwillow_zhvi_df_slice.date)\n",
    "\n",
    "# Aggregate data based on zip code and year\n",
    "zwillow_zhvi_zip_agg = zwillow_zhvi_df_slice.groupby(by=['zip', zwillow_zhvi_df_slice.date.dt.year]).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5035a923-6869-44ee-9bfe-bc046c91985c",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Aggregating Zip-Price Data from All Sources\n",
    "\n",
    "Extracted and transformed data from four sources are loaded into the final table:\n",
    "\n",
    "* Apprisal data from FHFA\n",
    "* Research data from redfin\n",
    "* Research data from realtor\n",
    "* Research data from Zwillow \n",
    "\n",
    "The final table is saved as a parquet file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9c165592-531b-42ed-9b30-ea932c22da11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Harmonizing column headers for joining\n",
    "multi_index_names = ['zip', 'year']\n",
    "zip_uad_df_merged.index.names = multi_index_names\n",
    "redfin_df_slice_zip_agg.index.names = multi_index_names\n",
    "realtor_df_slice_agg.index.names = multi_index_names\n",
    "zwillow_zhvi_zip_agg.index.names = multi_index_names\n",
    "\n",
    "# Adding suffix to column names to keep track of the source in the merged dataframe\n",
    "zip_uad_df_merged = zip_uad_df_merged.add_suffix('_uad')\n",
    "redfin_df_slice_zip_agg = redfin_df_slice_zip_agg.add_suffix('_redfin')\n",
    "realtor_df_slice_agg = realtor_df_slice_agg.add_suffix('_realtor')\n",
    "zwillow_zhvi_zip_agg = zwillow_zhvi_zip_agg.add_suffix('_zwillow')\n",
    "\n",
    "# Merging dataframe\n",
    "dfs_to_merge = [zip_uad_df_merged, redfin_df_slice_zip_agg, realtor_df_slice_agg, zwillow_zhvi_zip_agg]\n",
    "zip_price_master_df = reduce(lambda  left,right: pd.merge(left,right,left_index=True, right_index=True,\n",
    "                                            how='outer'), dfs_to_merge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9e948694-0da7-4b87-9bde-63a5c3d2727f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table:  house_price_table saved.\n"
     ]
    }
   ],
   "source": [
    "# Saving final table as parquet file\n",
    "out_dir = 'data/etl_data/zip_year_house_price_table'\n",
    "save_pd_to_parquet(dtframe=zip_price_master_df, fldr_name=out_dir, table_name='house_price_table')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d944aab-12f4-4775-aa34-03d857d01b4d",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Zip Code Details\n",
    "\n",
    "Source of data: http://download.geonames.org/export/zip/\n",
    "\n",
    "This table provides lattitude, longitude of each zip codes. This table can be used to map zip code(s) to specific lattitude(s), longitude(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "dc5eb29a-26da-446e-91a2-e1ff67a116f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already present.\n"
     ]
    }
   ],
   "source": [
    "import configparser\n",
    "\n",
    "config = configparser.ConfigParser()\n",
    "config.read('hpa.cfg')\n",
    "\n",
    "# Download Zip Code details\n",
    "\n",
    "zip_url = config.get('ZIPCODE', 'zip_url') \n",
    "\n",
    "fetch_file(url=zip_url, folder_name='data/raw_data/geonames/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b7c488e6-90f5-48ea-8d25-6b16b4a8a7d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data\n",
    "global_postcode_data = pd.read_csv('data/raw_data/geonames/allCountries.zip', sep='\\t', low_memory=False, header=None)\n",
    "column_headers = ['country_code', 'postal_code', 'place_name', 'admin_name1', 'admin_code1', 'admin_name2', 'admin_code2', 'admin_name3', 'admin_code3', 'latitude', 'longitude', 'accuracy']\n",
    "global_postcode_data.columns = column_headers\n",
    "\n",
    "# Getting only US zip codes\n",
    "us_postal_codes = global_postcode_data[global_postcode_data.country_code == 'US'].copy(deep=True)\n",
    "\n",
    "# Selecting relevant columns and renaming for clarity.\n",
    "us_postal_codes = us_postal_codes[['postal_code', 'place_name', 'admin_code1', 'latitude', 'longitude', 'accuracy']]\n",
    "us_postal_codes.columns = ['zip', 'city', 'state', 'latitude', 'longitude', 'accuracy']\n",
    "\n",
    "# Data type conversion\n",
    "us_postal_codes.zip = us_postal_codes.zip.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "839fedb2-5faa-4bf2-8f09-b8a44a14e159",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
